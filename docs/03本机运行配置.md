# 本机运行环境配置

## 本机运行环境配置

### 1. **安装 Miniconda**

Miniconda 是一个轻量级的 Conda 发行版，可以用来管理 Python 环境和包。它比 Anaconda 更加精简，适合需要定制化环境的用户。

1. **下载 Miniconda**

   访问 [Miniconda 官网](https://docs.conda.io/en/latest/miniconda.html) 下载适合你操作系统的安装包。对于 Ubuntu 64-bit 系统，可以下载 Linux 版本的安装包：

   ```bash
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   ```

2. **安装 Miniconda**

   运行下载的安装脚本：

   ```bash
   bash Miniconda3-latest-Linux-x86_64.sh
   ```

   按照提示完成安装。默认情况下，Miniconda 会安装在 `~/miniconda3` 目录。

3. **初始化 Miniconda**

   安装完成后，执行以下命令以便激活 `conda` 命令：

   ```bash
   conda init
   ```

   然后重启终端，或者执行以下命令重新加载 shell 配置：

   ```bash
   source ~/.bashrc
   ```

### 2. **创建虚拟环境**

在 Miniconda 中，使用 `conda` 来创建和管理虚拟环境。通过虚拟环境，你可以在同一台机器上运行不同版本的 Python 和包。

1. **创建虚拟环境**

   例如，创建一个名为 `myenv` 的虚拟环境，指定 Python 版本为 3.12：

   ```bash
   conda create -n myenv python=3.12
   ```

   你可以根据需要选择其他版本的 Python 或安装额外的包。

2. **激活虚拟环境**

   使用 `conda activate` 命令来激活刚刚创建的环境：

   ```bash
   conda activate myenv
   ```

   激活后，你会看到终端提示符前缀会变成虚拟环境的名称（如 `(myenv)`），表示当前工作在该环境中。

3. **安装其他依赖**

   在虚拟环境中，你可以通过 `conda install` 或 `pip install` 来安装所需的包。例如，安装常用的机器学习库：

   ```bash
   conda install numpy pandas scikit-learn
   ```

   或者使用 `pip` 安装不在 `conda` 仓库中的包：

   ```bash
   pip install tensorflow
   ```

4. **退出虚拟环境**

   完成工作后，可以通过以下命令退出虚拟环境：

   ```bash
   conda deactivate
   ```

### 3. **安装必要的软件**

此处参考大模型食用指南( https://github.com/datawhalechina/self-llm ) 。

首先 pip 换源加速下载并安装依赖包

# 升级pip
python -m pip install --upgrade pip
# 更换 pypi 源加速库的安装
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install requests==2.31.0
pip install fastapi==0.115.1
pip install uvicorn==0.30.6
pip install transformers==4.44.2
pip install huggingface-hub==0.25.0
pip install accelerate==0.34.2
pip install modelscope==1.18.0

### 4. 下载模型

此处参考大模型食用指南( https://github.com/datawhalechina/self-llm ) 。

模型下载
使用 modelscope 中的 snapshot_download 函数下载模型，第一个参数为模型名称，参数 cache_dir 为模型的下载路径。

新建 model_download.py 文件并在其中输入以下内容，粘贴代码后请及时保存文件，如下图所示。并运行 python model_download.py 执行下载，模型大小为 15GB，下载模型大概需要 5 分钟。

import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
注意：记得修改 cache_dir 为你的模型下载路径哦~

### 5. 语料准备

另文详述。此处从略。

### 6. 预训练

另文详述。此处从略。

### 7. vllm部署

此处参考大模型食用指南( https://github.com/datawhalechina/self-llm ) 。也可以参考qwen的官方文档 https://qwen.readthedocs.io/ 。 重点是如何用vllm部署自己增强训练之后的模型。以及如何进一步构建docker镜像部署。

### 8. Ollama运行

### 9. transformer部署

### 10. 参考内容

开源大模型食用指南 https://github.com/datawhalechina/self-llm/

Qwen官方文档 https://qwen.readthedocs.io/

Dify官方文档 https://docs.dify.ai/zh-hans

xinference文档 https://inference.readthedocs.io/

知乎全流程-本地 | 大模型调优、知识库的搭建和对话流程 LLaMA-Factory、Ollama、Langchain-Chatchat https://zhuanlan.zhihu.com/p/694306423

全流程-本地 | Windows 系统部署开源模型阿里通义千问 QWEN 1.5，结合 LangChain-Chatchat 框架和向量数据库 FAISS、Milvus https://zhuanlan.zhihu.com/p/690401615

数据处理方法参考 MNBVC(Massive Never-ending BT Vast Chinese corpus)超大规模中文语料集 https://github.com/esbatmop/MNBVC

模型训练使用llama-factory