# ai4ellm
## é¢„è®­ç»ƒè¯­æ–™åº“æ„å»ºæµç¨‹æ–‡æ¡£


## ğŸ“„ 1. PDF è½¬æ¢ï¼ˆpdf_converter.pyï¼‰

**åŠŸèƒ½è¯´æ˜ï¼š**  
å°†æŒ‡å®šæ–‡ä»¶å¤¹å†…çš„ `.docx`ã€`.pptx` ç­‰æ–‡ä»¶æ‰¹é‡è½¬æ¢ä¸º PDF æ–‡ä»¶ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
from comtypes.client import CreateObject
import os

def convert_to_pdf(input_path, output_path, file_type):
    if file_type in ['.docx', '.doc']:
        word = CreateObject('Word.Application')
        doc = word.Documents.Open(input_path)
        doc.SaveAs(output_path, FileFormat=17)
        doc.Close()
        word.Quit()
    elif file_type in ['.pptx', '.ppt']:
        powerpoint = CreateObject('PowerPoint.Application')
        presentation = powerpoint.Presentations.Open(input_path)
        presentation.SaveAs(output_path, 32)
        presentation.Close()
        powerpoint.Quit()

def batch_convert(folder_path):
    for root, _, files in os.walk(folder_path):
        for file in files:
            ext = os.path.splitext(file)[1].lower()
            if ext in ['.docx', '.doc', '.pptx', '.ppt']:
                input_file = os.path.join(root, file)
                output_file = os.path.splitext(input_file)[0] + '.pdf'
                convert_to_pdf(input_file, output_file, ext)
```

---

## ğŸ“¥ 2. PDF æå–ï¼ˆè½¬ Markdown,pdf_extractor.pyï¼‰

**åŠŸèƒ½è¯´æ˜ï¼š**  
å°† PDF æ–‡ä»¶å†…å®¹æå–å¹¶ä¿å­˜ä¸º Markdown æ–‡ä»¶ï¼Œæ”¯æŒ OCR è¯†åˆ«ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
def process_pdf(pdf_file_name, output_dir="output"):
    import torch
    from magic_pdf.data.dataset import PymuDocDataset

    ds = PymuDocDataset(pdf_file_name)
    parse_method = ds.classify()
    infer_result = ds.apply(ocr=(parse_method == "OCR"))
    infer_result.dump_md(output_dir)
```

---
---

## ğŸ“ 3. Markdown å†…å®¹æ¸…æ´—(markdown_cleaner.py)

**åŠŸèƒ½è¯´æ˜ï¼š**  
æ¸…æ´— Markdown å†…å®¹ï¼Œå»é™¤å›¾ç‰‡ã€é“¾æ¥ã€ç›®å½•ã€ä¹±ç å’Œå‚è€ƒæ–‡çŒ®ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
def clean_markdown(content):
    import re
    cleaned_content = []
    for line in content:
        line = re.sub(r"!\[.*?\]\(.*?\)", "", line)  # åˆ é™¤å›¾ç‰‡
        line = re.sub(r"\[.*?\]\(.*?\)", "", line)  # åˆ é™¤é“¾æ¥
        line = re.sub(r"\[\d+\]", "", line)          # åˆ é™¤æ–‡çŒ®å¼•ç”¨
        cleaned_content.append(line.strip())
    return cleaned_content
```

---
## ğŸ“¦ 4. txt æ–‡ä»¶åˆå¹¶(me.py)

**åŠŸèƒ½è¯´æ˜ï¼š**  
å°†å¤šä¸ª txt æˆ–æ–‡æœ¬æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­å¤„ç†ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
import os

def merge_txt_files(input_folder, output_file):
    with open(output_file, "w", encoding="utf-8") as outfile:
        for file_name in os.listdir(input_folder):
            if file_name.endswith(".txt"):
                file_path = os.path.join(input_folder, file_name)
                with open(file_path, "r", encoding="utf-8") as infile:
                    outfile.write(infile.read())
                    outfile.write("\n")
```



## ğŸ” 5. è¯­ä¹‰å»é‡(semantic_deduplicator.py)

**åŠŸèƒ½è¯´æ˜ï¼š**  
åŸºäº Sentence-BERT æ¨¡å‹ï¼Œè®¡ç®—å¥å­ç›¸ä¼¼åº¦å¹¶å»é‡ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
from sentence_transformers import SentenceTransformer
import numpy as np

def semantic_deduplicate(input_file, output_file, threshold=0.9):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    retained_embeddings = []
    with open(input_file, 'r') as fin, open(output_file, 'w') as fout:
        for line in fin:
            emb = model.encode(line.strip(), convert_to_numpy=True)
            if not retained_embeddings or max(np.dot(emb, np.vstack(retained_embeddings).T)) < threshold:
                fout.write(line)
                retained_embeddings.append(emb)
```

---

## ğŸ”„ 6. TXT è½¬ JSONL(txt_to_jsonl_converter.py)

**åŠŸèƒ½è¯´æ˜ï¼š**  
å°†æ¸…æ´—åçš„ `.txt` æ–‡ä»¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ `.jsonl` æ–‡ä»¶ï¼ŒæŒ‰ç« èŠ‚ä¿å­˜ã€‚

**æ ¸å¿ƒä»£ç ï¼š**
```python
import json
import os

def txt_to_jsonl(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for file_name in os.listdir(input_folder):
        if file_name.endswith(".txt"):
            with open(os.path.join(input_folder, file_name), 'r') as f:
                sections = f.read().split('\n# ')
            with open(os.path.join(output_folder, file_name.replace('.txt', '.jsonl')), 'w') as f:
                for section in sections:
                    title, *content = section.split('\n')
                    json.dump({"section": title, "content": "\n".join(content)}, f, ensure_ascii=False)
                    f.write('\n')
```


## ğŸ¯ **æ€»ç»“**

- **PDF è½¬æ¢** â†’ **PDF æå–** â†’ **æ–‡ä»¶åˆå¹¶** â†’ **Markdown æ¸…æ´—** â†’ **è¯­ä¹‰å»é‡** â†’ **JSONL è½¬æ¢**



